defmodule Cybernetic.Intelligence.HNSW.Index do
  @moduledoc """
  Hierarchical Navigable Small World (HNSW) index for fast vector similarity search.

  Implements approximate nearest neighbor search with:
  - Multi-layer graph structure
  - ETS-backed node storage for concurrent reads
  - Configurable M (max connections per node)
  - ef_construction for build quality
  - ef_search for query quality/speed tradeoff
  - Optional persistence via save/load

  ## Usage

      # Create index
      {:ok, _} = Index.start_link(dimensions: 384, m: 16)

      # Insert vectors
      :ok = Index.insert("doc_1", [0.1, 0.2, ...])

      # Batch insert (parallel)
      :ok = Index.insert_batch([{"doc_1", vec1}, {"doc_2", vec2}])

      # Search
      {:ok, results} = Index.search([0.15, 0.25, ...], k: 10)
      # => [{id: "doc_1", distance: 0.05, vector: [...]}]

      # Persistence
      :ok = Index.save("/path/to/index.bin")
      :ok = Index.load("/path/to/index.bin")
  """
  use GenServer

  require Logger

  @type vector :: [float()]
  @type node_id :: String.t()
  @type distance :: float()

  @type hnsw_node :: %{
          id: node_id(),
          vector: vector(),
          layer: non_neg_integer(),
          neighbors: %{non_neg_integer() => [node_id()]}
        }

  @type search_result :: %{
          id: node_id(),
          distance: distance(),
          vector: vector()
        }

  # Default HNSW parameters
  @default_m 16
  @default_ef_construction 200
  @default_ef_search 50
  @default_ml 1.0 / :math.log(16)
  @default_timeout 30_000
  @max_vector_dimensions 4096
  @max_nodes 10_000_000

  @telemetry [:cybernetic, :intelligence, :hnsw]

  # Client API

  @doc "Start the HNSW index"
  @spec start_link(keyword()) :: GenServer.on_start()
  def start_link(opts \\ []) do
    name = Keyword.get(opts, :name, __MODULE__)
    GenServer.start_link(__MODULE__, opts, name: name)
  end

  @doc "Insert a vector with ID"
  @spec insert(node_id(), vector(), keyword()) :: :ok | {:error, term()}
  def insert(id, vector, opts \\ []) do
    server = Keyword.get(opts, :server, __MODULE__)
    timeout = Keyword.get(opts, :timeout, @default_timeout)
    GenServer.call(server, {:insert, id, vector}, timeout)
  end

  @doc "Batch insert multiple vectors (parallel processing)"
  @spec insert_batch([{node_id(), vector()}], keyword()) :: :ok | {:error, term()}
  def insert_batch(items, opts \\ []) do
    server = Keyword.get(opts, :server, __MODULE__)
    timeout = Keyword.get(opts, :timeout, @default_timeout * 10)
    GenServer.call(server, {:insert_batch, items}, timeout)
  end

  @doc "Search for k nearest neighbors"
  @spec search(vector(), keyword()) :: {:ok, [search_result()]} | {:error, term()}
  def search(query_vector, opts \\ []) do
    server = Keyword.get(opts, :server, __MODULE__)
    k = Keyword.get(opts, :k, 10)
    ef = Keyword.get(opts, :ef, @default_ef_search)
    timeout = Keyword.get(opts, :timeout, @default_timeout)
    GenServer.call(server, {:search, query_vector, k, ef}, timeout)
  end

  @doc "Delete a vector by ID"
  @spec delete(node_id(), keyword()) :: :ok | {:error, :not_found}
  def delete(id, opts \\ []) do
    server = Keyword.get(opts, :server, __MODULE__)
    GenServer.call(server, {:delete, id})
  end

  @doc "Get index statistics"
  @spec stats(keyword()) :: map()
  def stats(opts \\ []) do
    server = Keyword.get(opts, :server, __MODULE__)
    GenServer.call(server, :stats)
  end

  @doc "Check if ID exists"
  @spec exists?(node_id(), keyword()) :: boolean()
  def exists?(id, opts \\ []) do
    server = Keyword.get(opts, :server, __MODULE__)
    GenServer.call(server, {:exists, id})
  end

  @doc "Get vector by ID"
  @spec get(node_id(), keyword()) :: {:ok, vector()} | {:error, :not_found}
  def get(id, opts \\ []) do
    server = Keyword.get(opts, :server, __MODULE__)
    GenServer.call(server, {:get, id})
  end

  @doc "Clear all vectors"
  @spec clear(keyword()) :: :ok
  def clear(opts \\ []) do
    server = Keyword.get(opts, :server, __MODULE__)
    GenServer.call(server, :clear)
  end

  @doc "Save index to file"
  @spec save(Path.t(), keyword()) :: :ok | {:error, term()}
  def save(path, opts \\ []) do
    server = Keyword.get(opts, :server, __MODULE__)
    GenServer.call(server, {:save, path}, @default_timeout * 10)
  end

  @doc "Load index from file"
  @spec load(Path.t(), keyword()) :: :ok | {:error, term()}
  def load(path, opts \\ []) do
    server = Keyword.get(opts, :server, __MODULE__)
    GenServer.call(server, {:load, path}, @default_timeout * 10)
  end

  # Server Callbacks

  @impl true
  def init(opts) do
    Logger.info("HNSW Index starting")

    # Create ETS table for concurrent read access
    nodes_table = :ets.new(:hnsw_nodes, [:set, :public, {:read_concurrency, true}])

    state = %{
      nodes_table: nodes_table,
      entry_point: nil,
      max_layer: 0,
      node_count: 0,
      dimensions: Keyword.get(opts, :dimensions, 384),
      m: Keyword.get(opts, :m, @default_m),
      m_max: Keyword.get(opts, :m_max, @default_m),
      m_max_0: Keyword.get(opts, :m_max_0, @default_m * 2),
      ef_construction: Keyword.get(opts, :ef_construction, @default_ef_construction),
      ml: Keyword.get(opts, :ml, @default_ml),
      stats: %{
        inserts: 0,
        searches: 0,
        deletes: 0,
        total_distance_computations: 0
      }
    }

    {:ok, state}
  end

  @impl true
  def handle_call({:insert, id, vector}, _from, state) do
    start_time = System.monotonic_time(:microsecond)

    with :ok <- validate_vector(vector, state.dimensions),
         :ok <- validate_capacity(state) do
      {new_state, distance_comps} = insert_node(state, id, vector)

      new_stats =
        new_state.stats
        |> Map.update!(:inserts, &(&1 + 1))
        |> Map.update!(:total_distance_computations, &(&1 + distance_comps))

      duration = System.monotonic_time(:microsecond) - start_time
      emit_telemetry(:insert, %{duration_us: duration, id: id})

      {:reply, :ok, %{new_state | stats: new_stats}}
    else
      {:error, _} = error ->
        {:reply, error, state}
    end
  end

  @impl true
  def handle_call({:insert_batch, items}, _from, state) do
    start_time = System.monotonic_time(:microsecond)

    # Validate all vectors first
    valid_items =
      Enum.filter(items, fn {_id, vector} ->
        validate_vector(vector, state.dimensions) == :ok
      end)

    # Check capacity
    if state.node_count + length(valid_items) > @max_nodes do
      {:reply, {:error, :max_nodes_reached}, state}
    else
      # Insert sequentially but with optimized single-GenServer-call
      # For truly parallel processing, would need distributed architecture
      new_state =
        Enum.reduce(valid_items, state, fn {id, vector}, acc ->
          {updated, _comps} = insert_node(acc, id, vector)
          update_in(updated, [:stats, :inserts], &(&1 + 1))
        end)

      duration = System.monotonic_time(:microsecond) - start_time
      emit_telemetry(:insert_batch, %{duration_us: duration, count: length(valid_items)})

      {:reply, :ok, new_state}
    end
  end

  @impl true
  def handle_call({:search, query_vector, k, ef}, _from, state) do
    start_time = System.monotonic_time(:microsecond)

    case validate_vector(query_vector, state.dimensions) do
      :ok when state.entry_point == nil ->
        {:reply, {:ok, []}, state}

      :ok ->
        {results, distance_comps} = search_knn(state, query_vector, k, ef)

        new_stats =
          state.stats
          |> Map.update!(:searches, &(&1 + 1))
          |> Map.update!(:total_distance_computations, &(&1 + distance_comps))

        duration = System.monotonic_time(:microsecond) - start_time
        emit_telemetry(:search, %{duration_us: duration, k: k, results: length(results)})

        {:reply, {:ok, results}, %{state | stats: new_stats}}

      {:error, _} = error ->
        {:reply, error, state}
    end
  end

  @impl true
  def handle_call({:delete, id}, _from, state) do
    case get_node(state, id) do
      nil ->
        {:reply, {:error, :not_found}, state}

      node ->
        # Remove from ETS
        :ets.delete(state.nodes_table, id)

        # Update neighbor lists to remove references
        update_neighbors_on_delete(state, id, node)

        # Update entry point if needed
        new_entry =
          if state.entry_point == id do
            case :ets.first(state.nodes_table) do
              :"$end_of_table" -> nil
              first_id -> first_id
            end
          else
            state.entry_point
          end

        new_stats = Map.update!(state.stats, :deletes, &(&1 + 1))

        {:reply, :ok,
         %{state | entry_point: new_entry, node_count: state.node_count - 1, stats: new_stats}}
    end
  end

  @impl true
  def handle_call(:stats, _from, state) do
    stats =
      state.stats
      |> Map.put(:node_count, state.node_count)
      |> Map.put(:max_layer, state.max_layer)
      |> Map.put(:dimensions, state.dimensions)
      |> Map.put(:m, state.m)
      |> Map.put(:ef_construction, state.ef_construction)

    {:reply, stats, state}
  end

  @impl true
  def handle_call({:exists, id}, _from, state) do
    exists = :ets.member(state.nodes_table, id)
    {:reply, exists, state}
  end

  @impl true
  def handle_call({:get, id}, _from, state) do
    case get_node(state, id) do
      nil -> {:reply, {:error, :not_found}, state}
      node -> {:reply, {:ok, node.vector}, state}
    end
  end

  @impl true
  def handle_call(:clear, _from, state) do
    :ets.delete_all_objects(state.nodes_table)

    new_state = %{
      state
      | entry_point: nil,
        max_layer: 0,
        node_count: 0
    }

    {:reply, :ok, new_state}
  end

  @impl true
  def handle_call({:save, path}, _from, state) do
    # Implicit try/catch via pattern matching
    with :ok <- validate_path(path),
         {:ok, binary} <- dump_state(state),
         :ok <- File.write(path, binary) do
      Logger.info("HNSW index saved", path: path)
      {:reply, :ok, state}
    else
      {:error, _} = error ->
        {:reply, error, state}
    end
  end

  @impl true
  def handle_call({:load, path}, _from, state) do
    # Implicit try/catch via pattern matching and guards
    with {:ok, binary} <- File.read(path),
         {:safe, data} <- {:safe, :erlang.binary_to_term(binary, [:safe])},
         :ok <- validate_loaded_data(data, state),
         :ok <- load_data_into_ets(state.nodes_table, data) do

      new_state = %{
        state
        | entry_point: data.entry_point,
          max_layer: data.max_layer,
          node_count: length(data.nodes)
      }

      Logger.info("HNSW index loaded", path: path, nodes: length(data.nodes))
      {:reply, :ok, new_state}
    else
      {:error, _} = error ->
        {:reply, error, state}
    end
  end

  @impl true
  def terminate(_reason, state) do
    :ets.delete(state.nodes_table)
    :ok
  end

  # --- Refactored Save/Load Logic Helpers ---

  defp validate_path(path) when is_binary(path), do: :ok
  defp validate_path(_), do: {:error, :invalid_path}

  defp dump_state(state) do
    nodes = :ets.tab2list(state.nodes_table)
    data = %{
      version: 1,
      dimensions: state.dimensions,
      m: state.m,
      entry_point: state.entry_point,
      max_layer: state.max_layer,
      nodes: nodes
    }
    {:ok, :erlang.term_to_binary(data, [:compressed])}
  rescue
    e -> {:error, Exception.message(e)}
  end

  defp validate_loaded_data(data, state) do
    cond do
      not is_map(data) -> {:error, :invalid_data_format}
      data.dimensions != state.dimensions -> {:error, :dimension_mismatch}
      true -> :ok
    end
  end

  defp load_data_into_ets(table, data) do
    :ets.delete_all_objects(table)
    :ets.insert(table, data.nodes)
    :ok
  rescue
    e -> {:error, Exception.message(e)}
  end

  # --- Private Functions - ETS helpers ---

  defp get_node(state, id) do
    case :ets.lookup(state.nodes_table, id) do
      [{^id, node}] -> node
      [] -> nil
    end
  end

  defp put_node(state, id, node) do
    :ets.insert(state.nodes_table, {id, node})
  end

  defp update_neighbors_on_delete(state, deleted_id, deleted_node) do
    # For each layer the deleted node was in, update its neighbors
    Enum.each(deleted_node.neighbors, fn {layer, neighbor_ids} ->
      Enum.each(neighbor_ids, fn neighbor_id ->
        case get_node(state, neighbor_id) do
          nil ->
            :ok

          neighbor ->
            updated_neighbors =
              Map.update(neighbor.neighbors, layer, [], fn nlist ->
                List.delete(nlist, deleted_id)
              end)

            put_node(state, neighbor_id, %{neighbor | neighbors: updated_neighbors})
        end
      end)
    end)
  end

  # --- Private Functions - HNSW Algorithm ---

  @spec insert_node(map(), node_id(), vector()) :: {map(), non_neg_integer()}
  defp insert_node(state, id, vector) do
    node_layer = random_layer(state.ml)

    node = %{
      id: id,
      vector: vector,
      layer: node_layer,
      neighbors: %{}
    }

    if state.entry_point == nil do
      put_node(state, id, node)
      {%{state | entry_point: id, max_layer: node_layer, node_count: 1}, 0}
    else
      {updated_node, distance_comps} =
        insert_into_graph(state, id, node, state.entry_point, state.max_layer)

      put_node(state, id, updated_node)

      {new_entry, new_max} =
        if node_layer > state.max_layer do
          {id, node_layer}
        else
          {state.entry_point, state.max_layer}
        end

      {%{state | entry_point: new_entry, max_layer: new_max, node_count: state.node_count + 1},
       distance_comps}
    end
  end

  # Refactored insert_into_graph to reduce nesting
  @spec insert_into_graph(map(), node_id(), hnsw_node(), node_id(), non_neg_integer()) ::
          {hnsw_node(), non_neg_integer()}
  defp insert_into_graph(state, new_id, new_node, entry_point, max_layer) do
    {current, distance_comps} =
      if max_layer > new_node.layer do
        Enum.reduce(max_layer..(new_node.layer + 1)//-1, {entry_point, 0}, fn layer,
                                                                               {curr, comps} ->
          if entry_node = get_node(state, curr) do
            {nearest, new_comps} = search_layer_greedy(state, new_node.vector, entry_node, layer)
            {nearest.id, comps + new_comps}
          else
            {curr, comps}
          end
        end)
      else
        {entry_point, 0}
      end

    {final_node, total_comps} =
      Enum.reduce(min(new_node.layer, max_layer)..0//-1, {new_node, distance_comps}, fn layer,
                                                                                        {node,
                                                                                         comps} ->
        {results, node, updated_comps} = process_layer_insert(state, new_id, node, layer, current, comps)
        {node, updated_comps + results}
      end)

    {final_node, total_comps}
  end

  # Extracted logic to reduce nesting inside insert_into_graph
  defp process_layer_insert(state, new_id, node, layer, current_id, comps) do
    with true <- (entry_node = get_node(state, current_id)) != nil,
         m_max when m_max > 0 <-