#!/usr/bin/env elixir

# Dogfooding Test for Cybernetic Optimization Modules
# Tests AMQP PublisherPool, CRDT Cache, Batched Telemetry, and Adaptive Circuit Breaker

Mix.install([
  {:cybernetic, path: "."}
])

defmodule DogfoodTest do
  @moduledoc """
  Real-world dogfooding test that exercises all optimization modules
  """
  
  require Logger
  
  def run do
    Logger.info("ðŸ• Starting Cybernetic Optimization Dogfood Test")
    Logger.info("=" |> String.duplicate(60))
    
    # Start the application
    {:ok, _} = Application.ensure_all_started(:cybernetic)
    Process.sleep(2000) # Let everything initialize
    
    # Run test scenarios
    test_amqp_publisher_pool()
    test_crdt_cache()
    test_batched_telemetry()
    test_adaptive_circuit_breaker()
    test_integrated_workflow()
    
    Logger.info("\nâœ… All dogfood tests completed successfully!")
  end
  
  defp test_amqp_publisher_pool do
    Logger.info("\nðŸ“¬ Testing AMQP Publisher Pool...")
    Logger.info("-" |> String.duplicate(40))
    
    alias Cybernetic.Core.Transport.AMQP.PublisherPool
    
    # Test async publishing with batching
    Logger.info("  â€¢ Testing async batch publishing...")
    for i <- 1..50 do
      PublisherPool.publish_async(
        "cyb.events",
        "test.dogfood",
        %{
          test_id: i,
          message: "Dogfood message #{i}",
          timestamp: DateTime.utc_now()
        }
      )
      
      if rem(i, 10) == 0 do
        Logger.info("    Published #{i} messages...")
      end
    end
    
    Process.sleep(100) # Let batch flush
    
    # Test sync publishing for critical messages
    Logger.info("  â€¢ Testing sync publishing...")
    result = PublisherPool.publish_sync(
      "cyb.commands",
      "critical.command",
      %{
        command: "emergency_stop",
        priority: "high",
        timestamp: DateTime.utc_now()
      }
    )
    
    Logger.info("    Sync publish result: #{inspect(result)}")
    
    # Test batch publish directly
    Logger.info("  â€¢ Testing direct batch publish...")
    batch = for i <- 1..20 do
      {"cyb.telemetry", "metrics.batch", %{metric_id: i, value: :rand.uniform(100)}, []}
    end
    
    {:ok, stats} = PublisherPool.batch_publish(batch)
    Logger.info("    Batch stats: #{inspect(stats)}")
    
    Logger.info("  âœ“ AMQP Publisher Pool test completed")
  end
  
  defp test_crdt_cache do
    Logger.info("\nðŸ’¾ Testing CRDT Cache...")
    Logger.info("-" |> String.duplicate(40))
    
    alias Cybernetic.Core.CRDT.Cache
    
    # Generate test data
    test_data = for i <- 1..100 do
      key = "test_key_#{i}"
      value = %{
        id: i,
        data: "Test data #{i}",
        random: :rand.uniform(1000)
      }
      {key, value}
    end
    
    # Test puts
    Logger.info("  â€¢ Storing 100 items in cache...")
    Enum.each(test_data, fn {key, value} ->
      Cache.put(key, value)
    end)
    
    # Test gets with hit/miss tracking
    Logger.info("  â€¢ Testing cache hits and misses...")
    hits = Enum.count(1..50, fn i ->
      key = "test_key_#{i}"
      case Cache.get(key) do
        {:ok, _value} -> true
        :not_found -> false
      end
    end)
    
    misses = Enum.count(1..50, fn i ->
      key = "nonexistent_key_#{i}"
      case Cache.get(key) do
        {:ok, _value} -> false
        :not_found -> true
      end
    end)
    
    Logger.info("    Hits: #{hits}/50, Misses: #{misses}/50")
    
    # Test TTL expiration
    Logger.info("  â€¢ Testing TTL expiration...")
    Cache.put("ttl_test", %{data: "expires soon"}, ttl: 100)
    {:ok, _} = Cache.get("ttl_test")
    Logger.info("    Item cached successfully")
    Process.sleep(150)
    result = Cache.get("ttl_test")
    Logger.info("    After TTL: #{inspect(result)}")
    
    # Test cache stats
    stats = Cache.get_stats()
    Logger.info("  â€¢ Cache stats: #{inspect(stats)}")
    
    Logger.info("  âœ“ CRDT Cache test completed")
  end
  
  defp test_batched_telemetry do
    Logger.info("\nðŸ“Š Testing Batched Telemetry Collector...")
    Logger.info("-" |> String.duplicate(40))
    
    alias Cybernetic.Telemetry.BatchedCollector
    
    # Add custom handler for testing
    handler_ref = make_ref()
    BatchedCollector.add_handler(handler_ref, fn batch ->
      Logger.info("    Custom handler received batch of #{length(batch)} events")
    end)
    
    # Generate telemetry events
    Logger.info("  â€¢ Generating telemetry events...")
    for i <- 1..150 do
      :telemetry.execute(
        [:cyb, :amqp, :publish],
        %{bytes: :rand.uniform(1000), latency_us: :rand.uniform(1000)},
        %{exchange: "test", routing_key: "test.#{i}"}
      )
      
      if rem(i, 50) == 0 do
        Logger.info("    Generated #{i} events...")
      end
    end
    
    # Force flush to see results
    Logger.info("  â€¢ Forcing batch flush...")
    BatchedCollector.flush()
    
    # Get collector stats
    Process.sleep(100)
    stats = BatchedCollector.get_stats()
    Logger.info("  â€¢ Collector stats: #{inspect(stats)}")
    
    # Clean up handler
    BatchedCollector.remove_handler(handler_ref)
    
    Logger.info("  âœ“ Batched Telemetry test completed")
  end
  
  defp test_adaptive_circuit_breaker do
    Logger.info("\nâš¡ Testing Adaptive Circuit Breaker...")
    Logger.info("-" |> String.duplicate(40))
    
    alias Cybernetic.Core.Resilience.AdaptiveCircuitBreaker
    
    # Start a circuit breaker
    {:ok, _pid} = AdaptiveCircuitBreaker.start_link(name: :dogfood_breaker)
    Process.sleep(100)
    
    # Test successful calls
    Logger.info("  â€¢ Testing successful calls...")
    success_results = for i <- 1..5 do
      AdaptiveCircuitBreaker.call(:dogfood_breaker, fn ->
        {:ok, "Success #{i}"}
      end)
    end
    Logger.info("    Results: #{inspect(success_results)}")
    
    # Test failures to trigger opening
    Logger.info("  â€¢ Testing circuit breaker opening...")
    failure_results = for i <- 1..6 do
      result = AdaptiveCircuitBreaker.call(:dogfood_breaker, fn ->
        raise "Simulated failure #{i}"
      end)
      
      case result do
        {:error, :circuit_breaker_open} ->
          Logger.info("    Circuit opened at failure #{i}")
        _ ->
          nil
      end
      
      result
    end
    
    # Check state
    state = AdaptiveCircuitBreaker.get_state(:dogfood_breaker)
    Logger.info("  â€¢ Circuit breaker state: #{inspect(state)}")
    
    # Test recovery
    Logger.info("  â€¢ Testing circuit recovery...")
    AdaptiveCircuitBreaker.force_state(:dogfood_breaker, :half_open)
    
    recovery_result = AdaptiveCircuitBreaker.call(:dogfood_breaker, fn ->
      {:ok, "Recovered!"}
    end)
    Logger.info("    Recovery result: #{inspect(recovery_result)}")
    
    final_state = AdaptiveCircuitBreaker.get_state(:dogfood_breaker)
    Logger.info("  â€¢ Final state: #{inspect(final_state)}")
    
    Logger.info("  âœ“ Adaptive Circuit Breaker test completed")
  end
  
  defp test_integrated_workflow do
    Logger.info("\nðŸ”„ Testing Integrated Workflow...")
    Logger.info("-" |> String.duplicate(40))
    Logger.info("  Simulating real-world VSM message flow with optimizations")
    
    alias Cybernetic.Core.Transport.AMQP.PublisherPool
    alias Cybernetic.Core.CRDT.Cache
    alias Cybernetic.Core.Resilience.AdaptiveCircuitBreaker
    
    # Start workflow circuit breaker
    {:ok, _} = AdaptiveCircuitBreaker.start_link(name: :workflow_breaker)
    
    # Simulate VSM S1 -> S2 -> S4 message flow
    Logger.info("\n  â€¢ Simulating VSM message flow...")
    
    workflow_task = fn ->
      # S1 Operation (with caching)
      operation_id = "op_#{:rand.uniform(10000)}"
      Cache.put("current_operation", %{id: operation_id, stage: "s1"})
      
      # Publish to S2 via pool
      PublisherPool.publish_async(
        "cyb.vsm.s2",
        "coordinate",
        %{
          operation_id: operation_id,
          source: "s1",
          action: "coordinate_resources"
        }
      )
      
      # Update cache
      Cache.put("current_operation", %{id: operation_id, stage: "s2"})
      
      # Simulate S2 -> S4 intelligence request
      PublisherPool.publish_async(
        "cyb.vsm.s4",
        "intelligence",
        %{
          operation_id: operation_id,
          source: "s2",
          request: "analyze_pattern"
        }
      )
      
      # Update cache
      Cache.put("current_operation", %{id: operation_id, stage: "s4"})
      
      # Generate telemetry
      :telemetry.execute(
        [:vsm, :s4, :intelligence],
        %{duration_ms: :rand.uniform(100)},
        %{operation_id: operation_id}
      )
      
      {:ok, operation_id}
    end
    
    # Run workflow with circuit breaker protection
    results = for i <- 1..10 do
      result = AdaptiveCircuitBreaker.call(:workflow_breaker, workflow_task, 5000)
      
      case result do
        {:ok, op_id} ->
          Logger.info("    âœ“ Workflow #{i} completed: #{op_id}")
        {:error, reason} ->
          Logger.info("    âœ— Workflow #{i} failed: #{inspect(reason)}")
      end
      
      Process.sleep(50)
      result
    end
    
    # Check final operation in cache
    case Cache.get("current_operation") do
      {:ok, op} ->
        Logger.info("\n  â€¢ Final cached operation: #{inspect(op)}")
      _ ->
        Logger.info("\n  â€¢ No operation in cache")
    end
    
    # Get workflow stats
    breaker_state = AdaptiveCircuitBreaker.get_state(:workflow_breaker)
    cache_stats = Cache.get_stats()
    
    Logger.info("\n  ðŸ“ˆ Integrated Workflow Stats:")
    Logger.info("    â€¢ Successful workflows: #{Enum.count(results, &match?({:ok, _}, &1))}/10")
    Logger.info("    â€¢ Circuit breaker health: #{breaker_state.health_score}")
    Logger.info("    â€¢ Cache hit ratio: #{cache_stats.hits}/(#{cache_stats.hits + cache_stats.misses})")
    
    Logger.info("\n  âœ“ Integrated workflow test completed")
  end
end

# Run the dogfood test
DogfoodTest.run()