#!/usr/bin/env elixir

IO.puts("ğŸš€ Quick Ollama S4 Provider Test")
IO.puts("=" |> String.duplicate(35))

# Test 1: Check Ollama server
IO.puts("\nğŸ¥ Checking Ollama Server...")

case HTTPoison.get("http://localhost:11434/api/tags", [], timeout: 5000) do
  {:ok, %{status: 200, body: body}} ->
    case Jason.decode(body) do
      {:ok, %{"models" => models}} when is_list(models) ->
        IO.puts("âœ… Ollama server healthy")
        IO.puts("  Available models: #{length(models)}")
        for model <- Enum.take(models, 3) do
          IO.puts("    â€¢ #{model["name"]} (#{div(model["size"], 1_000_000_000)} GB)")
        end
      _ ->
        IO.puts("âŒ Unexpected response format")
    end
  {:error, reason} ->
    IO.puts("âŒ Ollama not available: #{inspect(reason)}")
end

# Test 2: Generate with lightweight model
IO.puts("\nğŸ¤– Testing Generation with TinyLlama...")

payload = %{
  "model" => "tinyllama:latest",
  "prompt" => "Explain the benefit of local AI processing for privacy in exactly one sentence.",
  "stream" => false,
  "options" => %{
    "temperature" => 0.1,
    "num_predict" => 30
  }
}

case Jason.encode(payload) do
  {:ok, json} ->
    start_time = System.monotonic_time(:millisecond)
    
    case HTTPoison.post(
      "http://localhost:11434/api/generate",
      json,
      [{"Content-Type", "application/json"}],
      timeout: 30_000,
      recv_timeout: 30_000
    ) do
      {:ok, %{status: 200, body: body}} ->
        latency = System.monotonic_time(:millisecond) - start_time
        
        case Jason.decode(body) do
          {:ok, response} ->
            IO.puts("âœ… Generation successful!")
            IO.puts("  Model: #{response["model"]}")
            IO.puts("  Response: #{response["response"]}")
            IO.puts("  Tokens generated: #{response["eval_count"]}")
            IO.puts("  Latency: #{latency}ms")
            IO.puts("  Cost: $0.00 (local processing)")
            IO.puts("  Privacy: 100% (no data leaves your machine)")
            
          {:error, _} ->
            IO.puts("âŒ Failed to parse response")
        end
        
      {:ok, %{status: status}} ->
        IO.puts("âŒ HTTP #{status} error")
        
      {:error, reason} ->
        IO.puts("âŒ Request failed: #{inspect(reason)}")
    end
    
  {:error, _} ->
    IO.puts("âŒ Failed to encode request")
end

# Test 3: Privacy comparison
IO.puts("\nğŸ”’ Privacy & Cost Comparison")
IO.puts("-" |> String.duplicate(30))

IO.puts("Provider Comparison for Episode Analysis:")
IO.puts("")
IO.puts("ğŸ“Š Anthropic Claude:")
IO.puts("  â€¢ Cost: ~$0.003-0.015 per request")
IO.puts("  â€¢ Privacy: Data sent to Anthropic servers")
IO.puts("  â€¢ Latency: 2-5 seconds")
IO.puts("  â€¢ Strength: Deep reasoning")
IO.puts("")
IO.puts("ğŸ“Š OpenAI GPT:")
IO.puts("  â€¢ Cost: ~$0.002-0.010 per request")
IO.puts("  â€¢ Privacy: Data sent to OpenAI servers")
IO.puts("  â€¢ Latency: 1-3 seconds")
IO.puts("  â€¢ Strength: Code generation")
IO.puts("")
IO.puts("ğŸ“Š Ollama Local:")
IO.puts("  â€¢ Cost: $0.00 (local compute)")
IO.puts("  â€¢ Privacy: 100% local (no external API)")
IO.puts("  â€¢ Latency: 0.5-2 seconds (depends on hardware)")
IO.puts("  â€¢ Strength: Privacy & zero cost")

IO.puts("\nâœ… Ollama Integration Benefits:")
IO.puts("â€¢ Perfect for sensitive data (GDPR, HIPAA)")
IO.puts("â€¢ Zero API costs for high-volume processing")
IO.puts("â€¢ No rate limits or quotas")
IO.puts("â€¢ Works offline")
IO.puts("â€¢ Predictable latency")

IO.puts("\nğŸ¯ S4 Routing Strategy:")
IO.puts("â€¢ Privacy-critical episodes â†’ Ollama")
IO.puts("â€¢ Complex reasoning â†’ Anthropic + Ollama fallback")
IO.puts("â€¢ Code generation â†’ OpenAI + Anthropic fallback")
IO.puts("â€¢ High-volume batch â†’ Ollama (cost-effective)")

IO.puts("\nğŸ Ollama S4 Provider Test Complete!")